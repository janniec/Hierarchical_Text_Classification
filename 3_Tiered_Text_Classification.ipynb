{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be approaching this project 2 ways. \n",
    "1. Classic text classification, with 1 model predicting 20 newsgroups.\n",
    "2. Tiered text classification, with 1 model predicting 6 topics and then 6 models, each predicting the newsgroups associated with each topic. \n",
    "\n",
    "Below, is the tiered text classification.\n",
    "1. The Tier1 model will predict 1 of 6 topics - 'comp', 'misc', 'rec', 'sci', 'soc', or 'talk'.  \n",
    "    * Because only 1 newsgroup is associated with 'misc' all messages classified as 'misc' by the Tier1 model will automatically be assigned as 'misc.forsale' on Tier2. \n",
    "2. The messages classfied under the other 5 topics will pass to 1 of 5 Tier2 models. Each Tier2 model will classify the texts as 1 of the associated newsgroup. \n",
    "    * the 'comp' model will predict 'graphics', 'os_ms-windows_misc', 'sys_ibm_pc_hardware', 'sys_mac_hardware', or 'windows_x'.\n",
    "    * the 'rec' model will predict 'autos', 'motorcycles', 'sport_baseball', or 'sport_hockey'.\n",
    "    * the 'sci' model will predict 'crypt', 'electronics', 'med', or 'space'.\n",
    "    * the 'soc' model will predict 'religion_atheism', 'religion_christian', or 'religion_misc'.\n",
    "    * the 'talk' model will predict 'politics_guns', 'politics_mideast', or 'politics_misc'.\n",
    "3. At the end, the predicted Tier2 labels will be evaluated against the actual 20 Newgroup labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0] \n",
      "\n",
      "Numpy Version: 1.16.4\n",
      "Pandas Version: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "libraries = (('Numpy', np), ('Pandas', pd))\n",
    "\n",
    "print(\"Python Version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TxtFiles as tf\n",
    "import FTCommands as ftc\n",
    "import EvaluatePredictions as ep\n",
    "\n",
    "sys.path.append('../MyModules/')\n",
    "import KleptoFunctions as kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Import data from 1_Process_Data.ipynb.   \n",
    "Texts have been cleaned and processed.   \n",
    "Target labels have been organized and tiered.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: 2019.06.25_cleaned_newsgroups \n",
      "# of Folders: 5 \n",
      "Type: <class 'pandas.core.frame.DataFrame'> \n",
      "Len: 18752\n"
     ]
    }
   ],
   "source": [
    "data = kf.puking_file('2019.06.25_cleaned_newsgroups', data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rec': 3, 'comp': 1, 'talk': 6, 'sci': 4, 'soc': 5}\n"
     ]
    }
   ],
   "source": [
    "tier1_targets = dict(zip(data['tier1_label'], data['tier1_targets']))\n",
    "del tier1_targets['misc']\n",
    "print(tier1_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1 Sets\n",
    "The entire list of doc IDs will be split 3 ways - Training set, Validation Set, & Holdout Set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout\t 3751\n",
      "training\t 11250\n",
      "validation\t 3751\n"
     ]
    }
   ],
   "source": [
    "t1_ids = tf.training_validation_holdout_split(data['_id'], random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2 Sets\n",
    "The Tier1 Training and Validation Sets will be used again to train the Tier 2 models. They are split up by their Tier1 labels and then split into Training and Validation sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** comp ***\n",
      "training\t 2935\n",
      "validation\t 979\n",
      "*** rec ***\n",
      "training\t 2358\n",
      "validation\t 786\n",
      "*** sci ***\n",
      "training\t 2367\n",
      "validation\t 790\n",
      "*** soc ***\n",
      "training\t 1441\n",
      "validation\t 481\n",
      "*** talk ***\n",
      "training\t 1584\n",
      "validation\t 529\n"
     ]
    }
   ],
   "source": [
    "experiment_ids = list(t1_ids[[k for k in t1_ids.keys() if 'training' in k][0]])+\\\n",
    "list(t1_ids[[k for k in t1_ids.keys() if 'validation' in k][0]])\n",
    "t2_ids = tf.tier2_training_validation(data, experiment_ids, \\\n",
    "                                       list(tier1_targets.keys()), \\\n",
    "                                       random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Txt Files\n",
    "Teir 1 id sets are labeled with the Tier1 labels.   \n",
    "Teir 2 id sets are labeled with the Tier2 labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/holdout_19.txt\n",
      "data/T1_training_19.txt\n",
      "data/T1_validation_19.txt\n"
     ]
    }
   ],
   "source": [
    "for name_part in t1_ids.keys():\n",
    "    filepath = data_path+'%s.txt' % name_part\n",
    "    df = data[data['_id'].isin(t1_ids[name_part])]\n",
    "    tf.make_txtfile(filepath, df, 'tier1_targets')\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/comp_training_19.txt\n",
      "data/comp_validation_19.txt\n",
      "data/rec_training_19.txt\n",
      "data/rec_validation_19.txt\n",
      "data/sci_training_19.txt\n",
      "data/sci_validation_19.txt\n",
      "data/soc_training_19.txt\n",
      "data/soc_validation_19.txt\n",
      "data/talk_training_19.txt\n",
      "data/talk_validation_19.txt\n"
     ]
    }
   ],
   "source": [
    "for name_part in t2_ids.keys():\n",
    "    filepath = data_path+'%s.txt' % name_part\n",
    "    df = data[data['_id'].isin(t2_ids[name_part])]\n",
    "    tf.make_txtfile(filepath, df, 'tier2_targets')\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model\n",
    "   * ngram - max length of word ngram.    \n",
    "   * lr - step size to convergance.   \n",
    "   * dim - size of word vectors.   \n",
    "   * ws - size of the context window.  \n",
    "   * epoch - number of passes over the training data.  \n",
    "   * loss - loss function.  \n",
    "       * ns - negative sampling  \n",
    "       * hs - hierarchical softmax  \n",
    "       * softmax  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 3\n",
    "lr = 0.25\n",
    "dim = 200\n",
    "ws = 7\n",
    "epoch = 25\n",
    "loss = 'ns'\n",
    "\n",
    "train_filename = [k for k in t1_ids.keys() if 'training' in k][0]+'.txt'\n",
    "train_address = data_path+train_filename\n",
    "test_filename = [k for k in t1_ids.keys() if 'validation' in k][0]+'.txt'\n",
    "test_address = data_path+test_filename\n",
    "holdout_filename = [k for k in t1_ids.keys() if 'holdout' in k][0]+'.txt'\n",
    "holdout_address = data_path+holdout_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t3751\n",
      "P@1\t0.915\n",
      "R@1\t0.915\n",
      "\n",
      "data/T1_model\n",
      "data/T1_prediction_19.txt\n"
     ]
    }
   ],
   "source": [
    "model_address, predict_address = ftc.train_test_predict(train_address, test_address, \\\n",
    "                                                        ngram, lr, dim, ws, epoch, loss)\n",
    "print(model_address)\n",
    "print(predict_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note: FastText has its own evaluation and it is flawed. It is not precision, recall, accuracy, or f1 score, as you can see the comparison below. Evaluation with SkLearn is recommended.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9095798179935111\n",
      "recall: 0.8935651053144699\n",
      "fscore: 0.9005203360307982\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = ep.score_txtfiles(test_address, predict_address, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 2\n",
    "With the Tier 2 models split up into the 5 topics, we have an opportunity to tailor the parameters to each of the topic messages. (In order to find the best parameters to fit the model on the data, please use the ExperimentSweep module beforehand.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_label = 'comp'\n",
    "ngram = 2\n",
    "lr = 0.25\n",
    "dim = 200\n",
    "ws = 5\n",
    "epoch = 25\n",
    "loss = 'ns'\n",
    "T2_train_address = data_path+'%s_training_%s.txt' % (T1_label, str(random_state))\n",
    "T2_test_address = data_path+'%s_validation_%s.txt' % (T1_label, str(random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t979\n",
      "P@1\t0.837\n",
      "R@1\t0.837\n",
      "\n",
      "data/comp_model\n"
     ]
    }
   ],
   "source": [
    "T2_model_address, T2_predict_address = ftc.train_test_predict(T2_train_address, T2_test_address,\\\n",
    "                                                        ngram, lr, dim, ws, epoch, loss)\n",
    "print(T2_model_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_label = 'rec'\n",
    "ngram = 2\n",
    "lr = 0.5\n",
    "dim = 100\n",
    "ws = 3\n",
    "epoch = 25\n",
    "loss = 'ns'\n",
    "T2_train_address = data_path+'%s_training_%s.txt' % (T1_label, str(random_state))\n",
    "T2_test_address = data_path+'%s_validation_%s.txt' % (T1_label, str(random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t786\n",
      "P@1\t0.941\n",
      "R@1\t0.941\n",
      "\n",
      "data/rec_model\n"
     ]
    }
   ],
   "source": [
    "T2_model_address, T2_predict_address = ftc.train_test_predict(T2_train_address, T2_test_address,\\\n",
    "                                                        ngram, lr, dim, ws, epoch, loss)\n",
    "print(T2_model_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_label = 'sci'\n",
    "ngram = 3\n",
    "lr = 0.5\n",
    "dim = 200\n",
    "ws = 5\n",
    "epoch = 20\n",
    "loss = 'ns'\n",
    "T2_train_address = data_path+'%s_training_%s.txt' % (T1_label, str(random_state))\n",
    "T2_test_address = data_path+'%s_validation_%s.txt' % (T1_label, str(random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t790\n",
      "P@1\t0.951\n",
      "R@1\t0.951\n",
      "\n",
      "data/sci_model\n"
     ]
    }
   ],
   "source": [
    "T2_model_address, T2_predict_address = ftc.train_test_predict(T2_train_address, T2_test_address,\\\n",
    "                                                        ngram, lr, dim, ws, epoch, loss)\n",
    "print(T2_model_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_label = 'soc'\n",
    "ngram = 2 \n",
    "lr = 0.5\n",
    "dim = 200\n",
    "ws = 5\n",
    "epoch = 25\n",
    "loss = 'ns'\n",
    "T2_train_address = data_path+'%s_training_%s.txt' % (T1_label, str(random_state))\n",
    "T2_test_address = data_path+'%s_validation_%s.txt' % (T1_label, str(random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t481\n",
      "P@1\t0.819\n",
      "R@1\t0.819\n",
      "\n",
      "data/soc_model\n"
     ]
    }
   ],
   "source": [
    "T2_model_address, T2_predict_address = ftc.train_test_predict(T2_train_address, T2_test_address,\\\n",
    "                                                        ngram, lr, dim, ws, epoch, loss)\n",
    "print(T2_model_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_label = 'talk'\n",
    "ngram = 3\n",
    "lr = 0.5\n",
    "dim = 200\n",
    "ws = 5\n",
    "epoch = 25\n",
    "loss = 'ns'\n",
    "T2_train_address = data_path+'%s_training_%s.txt' % (T1_label, str(random_state))\n",
    "T2_test_address = data_path+'%s_validation_%s.txt' % (T1_label, str(random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t529\n",
      "P@1\t0.934\n",
      "R@1\t0.934\n",
      "\n",
      "data/talk_model\n"
     ]
    }
   ],
   "source": [
    "T2_model_address, T2_predict_address = ftc.train_test_predict(T2_train_address, T2_test_address,\\\n",
    "                                                        ngram, lr, dim, ws, epoch, loss)\n",
    "print(T2_model_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout set\n",
    "Let's see how our models do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tier1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_address = data_path+'T1_model'\n",
    "test_address = data_path+'holdout_%s.txt' % str(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t3751\n",
      "P@1\t0.92\n",
      "R@1\t0.92\n",
      "\n",
      "data/holdout_prediction_19.txt\n"
     ]
    }
   ],
   "source": [
    "predict_address = ftc.test_predict(test_address, model_address)\n",
    "print(predict_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9138092889691832\n",
      "recall: 0.8969858782681475\n",
      "fscore: 0.9042942326580093\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = ep.score_txtfiles(test_address, predict_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update df with Predictions\n",
    "We need to update the dataframe with the predicted Tier1 labels so that we can split the messages and pass them along to the appropriate Tier2 model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 7)\n"
     ]
    }
   ],
   "source": [
    "holdout_data = data[data['_id'].isin(t1_ids['holdout_'+str(random_state)])]\n",
    "print(holdout_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 8)\n"
     ]
    }
   ],
   "source": [
    "predict_labels = [int(p[9:]) for p in ep.collect_labels(predict_address)]\n",
    "holdout_data.insert(loc=0, column='tier1_predictions', value=predict_labels)\n",
    "print(holdout_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier2 Txt Files\n",
    "Now, we generate the Tier2 txtfiles with the holdout set, split up into their predicted Teir1 topics. For example. The Teir2 'comp' model will be tested with the messages in the holdout set that were predicted to be in the 'comp' topic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t1_name in tier1_targets:\n",
    "    filename = data_path+'holdout_%s.txt' % t1_name\n",
    "    df = holdout_data[holdout_data['tier1_predictions']==tier1_targets[t1_name]]\n",
    "    tf.make_txtfile(filename, df, 'tier2_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Models\n",
    "Please note: FastText has its own evaluation and it is flawed. It is not precision, recall, accuracy, or f1 score. We will perform final evaluation of the Tier2 predictions against the 20 newsgroup labels with SkLearn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** rec ***\n",
      "N\t783\n",
      "P@1\t0.954\n",
      "R@1\t0.954\n",
      "\n",
      "data/holdout_prediction_rec.txt\n",
      "\n",
      "*** comp ***\n",
      "N\t883\n",
      "P@1\t0.855\n",
      "R@1\t0.855\n",
      "\n",
      "data/holdout_prediction_comp.txt\n",
      "\n",
      "*** talk ***\n",
      "N\t478\n",
      "P@1\t0.941\n",
      "R@1\t0.941\n",
      "\n",
      "data/holdout_prediction_talk.txt\n",
      "\n",
      "*** sci ***\n",
      "N\t706\n",
      "P@1\t0.975\n",
      "R@1\t0.975\n",
      "\n",
      "data/holdout_prediction_sci.txt\n",
      "\n",
      "*** soc ***\n",
      "N\t452\n",
      "P@1\t0.841\n",
      "R@1\t0.841\n",
      "\n",
      "data/holdout_prediction_soc.txt\n"
     ]
    }
   ],
   "source": [
    "for t1_name in tier1_targets:\n",
    "    model_address = data_path+'%s_model' % t1_name\n",
    "    test_address = data_path+'holdout_%s.txt' % t1_name\n",
    "\n",
    "    print('\\n***', t1_name, '***')\n",
    "    predict_address = ftc.test_predict(test_address, model_address)\n",
    "    print(predict_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update df with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** rec ***\n",
      "(837, 9)\n",
      "*** comp ***\n",
      "(976, 9)\n",
      "*** talk ***\n",
      "(538, 9)\n",
      "*** sci ***\n",
      "(755, 9)\n",
      "*** soc ***\n",
      "(474, 9)\n"
     ]
    }
   ],
   "source": [
    "tier2_pred_dfs = {}\n",
    "for t1_name in tier1_targets:\n",
    "    print('***', t1_name, '***')\n",
    "    predict_address = data_path+'holdout_prediction_%s.txt' % t1_name   \n",
    "    predict_labels = [int(p[9:]) for p in ep.collect_labels(predict_address)]\n",
    "    df = holdout_data[holdout_data['tier1_predictions']==tier1_targets[t1_name]]\n",
    "    \n",
    "    df.insert(loc=0, column='tier2_predictions', value=predict_labels)\n",
    "    print(df.shape)\n",
    "    tier2_pred_dfs[t1_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = holdout_data[holdout_data['tier1_predictions']==2]\n",
    "df.insert(loc=0, column='tier2_predictions', value=21)\n",
    "tier2_pred_dfs['misc'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3751, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_data = pd.concat(tier2_pred_dfs.values())\n",
    "holdout_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout Set Final Evaluation\n",
    "Final evaluation of the Tier2 predictions against the 20 newsgroup labels with SkLearn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8427396283811962\n",
      "recall: 0.8381421947065771\n",
      "fscore: 0.8378384168620754\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = ep.score_columns(holdout_data, 'tier2_targets', 'tier2_predictions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
